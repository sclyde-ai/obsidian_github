- neural network
    ![IMG_7275.jpeg](IMG_7275.jpeg)
    - 
        where every neuron is connected to every neuron
    - CNN convolutional neural network
        - pooling layer
        - 畳み込み層 convolution layer
    - 例 example
        - 
        - 
        - 
        - Encoder-Decoder
        - attention
        - transformer
        - 
    - probability inference
- note
    - 
    - 
    - 
    - 
    https://qiita.com/konatsu/items/a016dbe136be9b626fff
- perceptron
    - and
        | x1 | x2 | y |
        | --- | --- | --- |
        | 0 | 0 | 0 |
        | 1 | 0 | 0 |
        | 0 | 1 | 0 |
        | 1 | 1 | 1 |
    - nand
        | x1 | x2 | y |
        | --- | --- | --- |
        | 0 | 0 | 1 |
        | 1 | 0 | 1 |
        | 0 | 1 | 1 |
        | 1 | 1 | 0 |
    - or
        | x1 | x2 | y |
        | --- | --- | --- |
        | 0 | 0 | 0 |
        | 1 | 0 | 1 |
        | 0 | 1 | 1 |
        | 1 | 1 | 1 |
    - xor
        | x1 | x2 | y |
        | --- | --- | --- |
        | 0 | 0 | 0 |
        | 1 | 0 | 1 |
        | 0 | 1 | 1 |
        | 1 | 1 | 0 |
- attention
- transformer
- seq2seq sequence to sequence
- MAMBA
- neural ordinary differential equation
- batch normalization
- optimization
    - 誤差逆伝播法 back propagation
    - 最急降下法
    - 確率的勾配降下法 SGD stochastic gradient descent
    - solomonoff’s induction